{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation with Data\n",
    "In this notebook, we introduce built-in evaluators and guide you through creating your own custom evaluators. We'll cover both code-based and prompt-based custom evaluators. Finally, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e99e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing any old installation\n",
    "# This is important since older version of promptflow has one package.\n",
    "# Now it is split into number of them.\n",
    "! pip uninstall -y promptflow promptflow-cli promptflow-azure promptflow-core promptflow-devkit promptflow-tools promptflow-evals\n",
    "\n",
    "# Install packages in this order\n",
    "! pip install promptflow-evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1714204",
   "metadata": {},
   "source": [
    "## Evaluate the eval dataset using the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31004ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment_name=\"vampire-bats\"\n",
    "experiment_dir=f\"dataset/{experiment_name}-files\"\n",
    "\n",
    "dataset_path_hf_eval = f\"{experiment_dir}/{experiment_name}-hf.eval.jsonl\"\n",
    "dataset_path_hf_eval_answer = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.jsonl\"\n",
    "dataset_path_hf_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.baseline.jsonl\"\n",
    "\n",
    "dataset_path_ft_eval = f\"{experiment_dir}/{experiment_name}-ft.eval.jsonl\"\n",
    "dataset_path_ft_eval_baseline = f\"{experiment_dir}/{experiment_name}-ft.eval.baseline.jsonl\"\n",
    "dataset_path_ft_eval_score = f\"{experiment_dir}/{experiment_name}-ft.eval.score.jsonl\"\n",
    "\n",
    "EVAL_OPENAI_BASE_URL_BASE = os.getenv('EVAL_OPENAI_BASE_URL_BASE')\n",
    "EVAL_OPENAI_API_KEY_BASE = os.getenv('EVAL_OPENAI_API_KEY_BASE')\n",
    "EVAL_OPENAI_DEPLOYMENT_BASE = os.getenv('EVAL_OPENAI_DEPLOYMENT_BASE')\n",
    "\n",
    "EVAL_OPENAI_BASE_URL_FT = os.getenv('EVAL_OPENAI_BASE_URL_FT')\n",
    "EVAL_OPENAI_API_KEY_FT = os.getenv('EVAL_OPENAI_API_KEY_FT')\n",
    "EVAL_OPENAI_DEPLOYMENT_FT = os.getenv('EVAL_OPENAI_DEPLOYMENT_FT')\n",
    "\n",
    "def obfuscate(secret):\n",
    "    l = len(secret)\n",
    "    return '.' * (l - 4) + secret[-4:]\n",
    "\n",
    "print(f\"EVAL_OPENAI_BASE_URL_BASE={EVAL_OPENAI_BASE_URL_BASE}\")\n",
    "print(f\"EVAL_OPENAI_API_KEY_BASE={obfuscate(EVAL_OPENAI_API_KEY_BASE)}\")\n",
    "print(f\"EVAL_OPENAI_DEPLOYMENT_BASE={EVAL_OPENAI_DEPLOYMENT_BASE}\")\n",
    "\n",
    "print(f\"EVAL_OPENAI_BASE_URL_FT={EVAL_OPENAI_BASE_URL_FT}\")\n",
    "print(f\"EVAL_OPENAI_API_KEY_FT={obfuscate(EVAL_OPENAI_API_KEY_FT)}\")\n",
    "print(f\"EVAL_OPENAI_DEPLOYMENT_FT={EVAL_OPENAI_DEPLOYMENT_FT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = input()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451ac45",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset AZURE_OPENAI_ENDPOINT && \\\n",
    "unset AZURE_OPENAI_API_KEY && \\\n",
    "unset OPENAI_API_VERSION && \\\n",
    "OPENAI_BASE_URL=$EVAL_OPENAI_BASE_URL_FT \\\n",
    "OPENAI_API_KEY=$EVAL_OPENAI_API_KEY_BASE \\\n",
    "python ../eval.py \\\n",
    "    --question-file $dataset_path_hf_eval \\\n",
    "    --answer-file $dataset_path_hf_eval_answer_baseline \\\n",
    "    --model $EVAL_OPENAI_DEPLOYMENT_BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5fe62",
   "metadata": {},
   "source": [
    "### Fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85194f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset AZURE_OPENAI_ENDPOINT && \\\n",
    "unset AZURE_OPENAI_API_KEY && \\\n",
    "unset OPENAI_API_VERSION && \\\n",
    "OPENAI_BASE_URL=$EVAL_OPENAI_BASE_URL_FT \\\n",
    "OPENAI_API_KEY=$EVAL_OPENAI_API_KEY_FT \\\n",
    "python ../eval.py \\\n",
    "    --question-file $dataset_path_hf_eval \\\n",
    "    --answer-file $dataset_path_hf_eval_answer \\\n",
    "    --model $EVAL_OPENAI_DEPLOYMENT_FT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee20226",
   "metadata": {},
   "source": [
    "## 0. Prepare eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../format.py \\\n",
    "    --input $dataset_path_hf_eval_answer \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_eval \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c84e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../format.py \\\n",
    "    --input $dataset_path_hf_eval_answer_baseline \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_ft_eval_baseline \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff092e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_path_ft_eval, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_ft_eval_baseline, lines=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "## 1. Built-in Evaluators\n",
    "\n",
    "The table below lists all the built-in evaluators we support. In the following sections, we will select a few of these evaluators to demonstrate how to use them.\n",
    "\n",
    "| Category       | Namespace                                        | Evaluator Class           | Notes                                             |\n",
    "|----------------|--------------------------------------------------|---------------------------|---------------------------------------------------|\n",
    "| Quality        | promptflow.evals.evaluators                      | GroundednessEvaluator     | Measures how well the answer is entailed by the context and is not hallucinated |\n",
    "|                |                                                  | RelevanceEvaluator        | How well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. |\n",
    "|                |                                                  | CoherenceEvaluator        | How well all the sentences fit together and sound naturally as a whole. |\n",
    "|                |                                                  | FluencyEvaluator          | Quality of individual sentences in the answer, and whether they are well-written and grammatically correct. |\n",
    "|                |                                                  | SimilarityEvaluator       | Measures the similarity between the predicted answer and the correct answer |\n",
    "|                |                                                  | F1ScoreEvaluator          | F1 score |\n",
    "| Content Safety | promptflow.evals.evaluators.content_safety       | ViolenceEvaluator         |                                                   |\n",
    "|                |                                                  | SexualEvaluator           |                                                   |\n",
    "|                |                                                  | SelfHarmEvaluator         |                                                   |\n",
    "|                |                                                  | HateUnfairnessEvaluator   |                                                   |\n",
    "| Composite      | promptflow.evals.evaluators                      | QAEvaluator               | Built on top of individual quality evaluators.    |\n",
    "|                |                                                  | ChatEvaluator             | Similar to QAEvaluator but designed for evaluating chat messages. |\n",
    "|                |                                                  | ContentSafetyEvaluator    | Built on top of individual content safety evaluators. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "### 1.1 Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd8617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "azure_endpoint=os.environ.get(\"EVAL_AZURE_OPENAI_ENDPOINT_EVALUATORS\")\n",
    "api_key=os.environ.get(\"EVAL_AZURE_OPENAI_API_KEY_EVALUATORS\")\n",
    "azure_deployment=os.environ.get(\"EVAL_AZURE_OPENAI_DEPLOYMENT_EVALUATORS\")\n",
    "api_version=os.environ.get(\"EVAL_OPENAI_API_VERSION_EVALUATORS\")\n",
    "\n",
    "print(\"azure_endpoint=\" + azure_endpoint)\n",
    "print(\"azure_deployment=\" + azure_deployment)\n",
    "print(\"api_version=\" + api_version)\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "# Initialzing Relevance Evaluator\n",
    "relevance_eval = RelevanceEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=df.iloc[1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf075450-1835-4b15-8002-76ae249caab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Relevance Evaluator on single input row\n",
    "relevance_score = relevance_eval(\n",
    "    question=sample['question'],\n",
    "    answer=sample['final_answer'],\n",
    "    context=sample['context'],\n",
    "    ground_truth=sample['gold_answer'],\n",
    ")\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d26663",
   "metadata": {},
   "source": [
    "## 3. Batch evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_path_ft_eval, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!AZURE_OPENAI_ENDPOINT=$azure_endpoint \\\n",
    "    AZURE_OPENAI_API_KEY=$api_key \\\n",
    "    AZURE_OPENAI_DEPLOYMENT=$azure_deployment \\\n",
    "    OPENAI_API_VERSION=$api_version \\\n",
    "    python ../pfeval.py \\\n",
    "    --input $dataset_path_ft_eval \\\n",
    "    --output $dataset_path_ft_eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_path_ft_eval_score, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "## 3. Using Evaluate API to evaluate with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2265e",
   "metadata": {},
   "source": [
    "First, let's take a peek at what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using a few evaluators that we already initialized\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=dataset_path_ft_eval,\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"answer\": \"${data.gold_answer}\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "from IPython.display import display, JSON\n",
    "display(JSON(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb28f9a-1f9c-4c2e-8b32-e7da51585f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "print(result[\"studio_url\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

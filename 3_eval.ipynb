{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6018523f-3425-4bb3-9810-d31b8912991c",
   "metadata": {},
   "source": [
    "# Evaluation of the fine-tuned and baseline models with the RAFT generated eval dataset split\n",
    "\n",
    "In this notebook, we will use the evaluation dataset synthetically generated in the [](./0_gen.ipynb) notebook using the RAFT method to evaluate both the baseline model and the fine-tuned model, then compare the two to analyse the impact of the fine-tuning.\n",
    "\n",
    "We introduce the `promptflow-evals` package and built-in evaluators. Then, we'll demonstrate how to use the `evaluate` API to assess data using these evaluators.\n",
    "\n",
    "Finally, we'll draw a diagram showing the performance of the fine-tuned model against the baseline.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Testing\n",
    "  - Run the baseline model on the evaluation split to get its predictions.\n",
    "  - Run the finetuned model on the evaluation split to get its predictions.\n",
    "- Answers formatting\n",
    "  - Convert the baseline model answers to a format suitable for testing\n",
    "  - Convert the fine-tuned model answers to a format suitable for testing\n",
    "- Evaluation\n",
    "  - Calculate the metrics (such as accuracy, precision, recall, etc.) based on the predictions from the baseline model.\n",
    "  - Calculate the metrics based on the predictions from the finetuned model.  \n",
    "- Compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e99e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install promptflow-evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1714204",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Define variables we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31004ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# User provided values\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Variables passed by previous notebooks\n",
    "load_dotenv('.env.state')\n",
    "\n",
    "# Let's capture the initial working directory because the evaluate function will change it\n",
    "dir = os.getcwd()\n",
    "\n",
    "experiment_name=os.getenv(\"DATASET_NAME\")\n",
    "experiment_dir=f\"{dir}/dataset/{experiment_name}-files\"\n",
    "\n",
    "# Dataset generated by the gen notebook that we will evaluate the baseline and finetuned models on\n",
    "dataset_path_hf_eval = f\"{experiment_dir}/{experiment_name}-hf.eval.jsonl\"\n",
    "\n",
    "# Evaluated answer files\n",
    "dataset_path_hf_eval_answer = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.jsonl\"\n",
    "dataset_path_hf_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-hf.eval.answer.baseline.jsonl\"\n",
    "\n",
    "# Formatted answer evaluation files\n",
    "dataset_path_eval_answer_finetuned = f\"{experiment_dir}/{experiment_name}-eval.answer.finetuned.jsonl\"\n",
    "dataset_path_eval_answer_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.baseline.jsonl\"\n",
    "\n",
    "# Scored answer files\n",
    "dataset_path_eval_answer_score_finetuned = f\"{experiment_dir}/{experiment_name}-eval.answer.score.finetuned.jsonl\"\n",
    "dataset_path_eval_answer_score_baseline = f\"{experiment_dir}/{experiment_name}-eval.answer.score.baseline.jsonl\"\n",
    "\n",
    "BASELINE_OPENAI_DEPLOYMENT = os.getenv(\"BASELINE_OPENAI_DEPLOYMENT\")\n",
    "FINETUNED_OPENAI_DEPLOYMENT = os.getenv(\"FINETUNED_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "print(f\"Evaluating the finetuned model {FINETUNED_OPENAI_DEPLOYMENT} against the baseline model {BASELINE_OPENAI_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451ac45",
   "metadata": {},
   "source": [
    "### Run the baseline model on the evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python .gorilla/raft/eval.py \\\n",
    "    --question-file $dataset_path_hf_eval \\\n",
    "    --answer-file $dataset_path_hf_eval_answer_baseline \\\n",
    "    --model $BASELINE_OPENAI_DEPLOYMENT \\\n",
    "    --env-prefix BASELINE \\\n",
    "    --mode chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5fe62",
   "metadata": {},
   "source": [
    "### Run the fine tuned model on the evaluation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85194f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python .gorilla/raft/eval.py \\\n",
    "    --question-file $dataset_path_hf_eval \\\n",
    "    --answer-file $dataset_path_hf_eval_answer \\\n",
    "    --model $FINETUNED_OPENAI_DEPLOYMENT \\\n",
    "    --env-prefix FINETUNED \\\n",
    "    --mode completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee20226",
   "metadata": {},
   "source": [
    "## Answers formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56cc46",
   "metadata": {},
   "source": [
    "### Format baseline answers\n",
    "\n",
    "Convert the baseline model answers to a format suitable for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c84e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_eval_answer_baseline \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_eval_answer_baseline \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b4d8b",
   "metadata": {},
   "source": [
    "### Format finetuned model answers\n",
    "\n",
    "Convert the fine-tuned model answers to a format suitable for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python .gorilla/raft/format.py \\\n",
    "    --input $dataset_path_hf_eval_answer \\\n",
    "    --input-type jsonl \\\n",
    "    --output $dataset_path_eval_answer_finetuned \\\n",
    "    --output-format eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a71e0",
   "metadata": {},
   "source": [
    "## Let's review the formatted files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c556e",
   "metadata": {},
   "source": [
    "### Finetuned model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c88ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff092e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_finetuned, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b966d32",
   "metadata": {},
   "source": [
    "### Baseline model answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_baseline, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcb5a0",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Built-in Evaluators\n",
    "\n",
    "The table below lists all the built-in evaluators we support. In the following sections, we will select a few of these evaluators to demonstrate how to use them.\n",
    "\n",
    "| Category       | Namespace                                        | Evaluator Class           | Notes                                             |\n",
    "|----------------|--------------------------------------------------|---------------------------|---------------------------------------------------|\n",
    "| Quality        | promptflow.evals.evaluators                      | GroundednessEvaluator     | Measures how well the answer is entailed by the context and is not hallucinated |\n",
    "|                |                                                  | RelevanceEvaluator        | How well the answer addresses the main aspects of the question, based on the context. Consider whether all and only the important aspects are contained in the answer when evaluating relevance. |\n",
    "|                |                                                  | CoherenceEvaluator        | How well all the sentences fit together and sound naturally as a whole. |\n",
    "|                |                                                  | FluencyEvaluator          | Quality of individual sentences in the answer, and whether they are well-written and grammatically correct. |\n",
    "|                |                                                  | SimilarityEvaluator       | Measures the similarity between the predicted answer and the correct answer |\n",
    "|                |                                                  | F1ScoreEvaluator          | F1 score |\n",
    "| Content Safety | promptflow.evals.evaluators.content_safety       | ViolenceEvaluator         |                                                   |\n",
    "|                |                                                  | SexualEvaluator           |                                                   |\n",
    "|                |                                                  | SelfHarmEvaluator         |                                                   |\n",
    "|                |                                                  | HateUnfairnessEvaluator   |                                                   |\n",
    "| Composite      | promptflow.evals.evaluators                      | QAEvaluator               | Built on top of individual quality evaluators.    |\n",
    "|                |                                                  | ChatEvaluator             | Similar to QAEvaluator but designed for evaluating chat messages. |\n",
    "|                |                                                  | ContentSafetyEvaluator    | Built on top of individual content safety evaluators. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f72d3d",
   "metadata": {},
   "source": [
    "#### Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a219acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "azure_endpoint = os.environ.get(\"SCORE_AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.environ.get(\"SCORE_AZURE_OPENAI_API_KEY\")\n",
    "azure_deployment = os.environ.get(\"SCORE_AZURE_OPENAI_DEPLOYMENT\")\n",
    "api_version = os.environ.get(\"SCORE_OPENAI_API_VERSION\")\n",
    "\n",
    "print(\"azure_endpoint=\" + azure_endpoint)\n",
    "print(\"azure_deployment=\" + azure_deployment)\n",
    "print(\"api_version=\" + api_version)\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluators import RelevanceEvaluator, SimilarityEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Initializing evaluators\n",
    "similarity = SimilarityEvaluator(model_config)\n",
    "groundedness = GroundednessEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6247d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(dataset_path_eval_answer_finetuned, lines=True)\n",
    "sample=df.iloc[1]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c67a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Groundedness Evaluator on single input row\n",
    "groundedness_score = groundedness(\n",
    "    answer=sample['final_answer'],\n",
    "    context=sample['context'],\n",
    ")\n",
    "print(groundedness_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d90565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Similarity Evaluator on single input row\n",
    "similarity_score = similarity(\n",
    "    question=sample['question'],\n",
    "    answer=sample['final_answer'],\n",
    "    context=sample['context'],\n",
    "    ground_truth=sample['gold_final_answer'],\n",
    ")\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846babb1-59b5-4d38-bb3a-d6eebd39ebee",
   "metadata": {},
   "source": [
    "### Using the Evaluate API to calculate the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe3fa2",
   "metadata": {},
   "source": [
    "In previous sections, we walked you through how to use built-in evaluators to evaluate a single row and how to define your own custom evaluators. Now, we will show you how to use these evaluators with the powerful `evaluate` API to assess an entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad968a0",
   "metadata": {},
   "source": [
    "### Configure AI Studio reporting (Optional)\n",
    "\n",
    "You can optional setup uploading the evaluation report to Azure AI Studio, to keep track of the evaluations and share with your team. In order to do so, configure the following environment variables with the information of the Azure AI Studio project you want to upload the reports to:\n",
    "\n",
    "```\n",
    "REPORT_SUB_ID=<SUBSCRIPTION ID>\n",
    "REPORT_GROUP=<RESOURCE GROUP NAME>\n",
    "REPORT_PROJECT_NAME=<AZURE AI STUDIO HUB PROJECT NAME>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e07cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_reporting_project_scope\n",
    "project_scope_report = get_reporting_project_scope()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049643c",
   "metadata": {},
   "source": [
    "### Running the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfc57",
   "metadata": {},
   "source": [
    "Now, we will invoke the `evaluate` API using a few evaluators that we already initialized\n",
    "\n",
    "Additionally, we have a column mapping to map the `truth` column from the dataset to `ground_truth`, which is accepted by the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b03e557-19bd-4a2a-ae3f-bbaf6846fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "def score_dataset(dataset, output_path=None):\n",
    "    result = evaluate(\n",
    "        data=dataset,\n",
    "        evaluators={\n",
    "            \"similarity\": similarity,\n",
    "            \"groundedness\": groundedness\n",
    "        },\n",
    "        azure_ai_project=project_scope_report,\n",
    "        # column mapping\n",
    "        evaluator_config={\n",
    "            \"similarity\": {\n",
    "                \"question\": \"${data.question}\",\n",
    "                \"answer\": \"${data.final_answer}\",\n",
    "                \"ground_truth\": \"${data.gold_final_answer}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "            },\n",
    "            \"groundedness\": {\n",
    "                \"answer\": \"${data.final_answer}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if output_path:\n",
    "        pd.DataFrame.from_dict(result['rows']).to_json(output_path, orient=\"records\", lines=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51dd4c",
   "metadata": {},
   "source": [
    "#### Baseline model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4059934",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_baseline, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_result = score_dataset(dataset_path_eval_answer_baseline, dataset_path_eval_answer_score_baseline)\n",
    "from IPython.display import display, JSON\n",
    "display(JSON(baseline_result['metrics']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "if baseline_result[\"studio_url\"]:\n",
    "    print(f\"Results uploaded to AI Studio {baseline_result['studio_url']}\")\n",
    "else:\n",
    "    print(\"Results available at http://127.0.0.1:23333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404ec34",
   "metadata": {},
   "source": [
    "#### Finetuned model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6185474",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(dataset_path_eval_answer_finetuned, lines=True).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3367eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_result = score_dataset(dataset_path_eval_answer_finetuned, dataset_path_eval_answer_score_finetuned)\n",
    "from IPython.display import display, JSON\n",
    "display(JSON(finetune_result['metrics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0fb00",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the results produced by the evaluate API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb28f9a-1f9c-4c2e-8b32-e7da51585f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results using Azure AI Studio UI\n",
    "if finetune_result[\"studio_url\"]:\n",
    "    print(f\"Results uploaded to AI Studio {finetune_result['studio_url']}\")\n",
    "else:\n",
    "    print(\"Results available at http://127.0.0.1:23333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79181e85",
   "metadata": {},
   "source": [
    "## Compare the metrics of the fine-tuned model against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325762f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame.from_dict({\"baseline\": baseline_result['metrics'], \"finetuned\": finetune_result['metrics']})\n",
    "metrics['improvement'] = metrics['finetuned'] / metrics['baseline']\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0669cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.drop('improvement', axis=1).plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1d349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
